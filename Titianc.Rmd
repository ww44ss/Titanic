---
title: "Titanic"
author: "Winston Saunders"
date: "December 31, 2014"
output: html_document
---

###Introduction

This is a model for the kaggle Titanic dataset. The idea is to make a predictive model of survivor. Judging from leaderboards, 


```{r "get data", echo=9:10}

## this section gets the data and loads it into training and testing data files

        ##set working directory

        setwd("/Users/winstonsaunders/Documents/TitanicKaggle")

        ## get data sets
        training <- read.csv("train.csv", na.strings=c("NA", ""))
        testing <- read.csv("test.csv", na.strings=c("NA", ""))

        trainingRaw<-training
        testingRaw<-testing

```

##First model 

Base first model on Sex and Age per tutorial. Since there are many NA's in the data we replace those with the mean ages for male or females.  
  
The model is trained with a _Random Forest_. 

```{r "clean data1", echo=FALSE}

  ## Clean the data

        ## calculate the median age for the population, men and women
        meanAge<-mean(training$Age[is.na(training$Age)==FALSE])

        meanMaleAge<-mean(training$Age[training$Sex=="male"&is.na(training$Age)==FALSE])
        meanFemaleAge<-mean(training$Age[training$Sex=="female"&is.na(training$Age)==FALSE])

        ## substitute median ages based on sex
        
                training$Age[is.na(training$Age)&training$Sex=="male"]<-meanMaleAge
                training$Age[is.na(training$Age)&training$Sex=="female"]<-meanFemaleAge


                testing$Age[is.na(testing$Age)&testing$Sex=="male"]<-meanMaleAge
                testing$Age[is.na(testing$Age)&testing$Sex=="female"]<-meanFemaleAge


```
The mean male age is `r round(meanMaleAge, 1)` years and the mean female age is `r round(meanFemaleAge,1)` years. Here is a snapshot of the cleaned data.  Note the fractional year on substituted values. 

```{r}
training[1:6,c("Survived", "Age", "Sex")]
```

The first model looks at the following function

```{r "First model", warning=FALSE, echo=33, message=FALSE}

## This is a first simple model


      ## create data sub partition

        require(caret)
        set.seed(8675309)
        xx <- createDataPartition(y=training$Survived, p=0.80, list=FALSE)
   


        ##Define training and test sets

        train1<-training[xx,]
        test1<-training[-xx,]

        #train1 <- train1[,c("Survived","Sex", "Age")]
        #test1 <-   test1[,c("Survived","Sex", "Age")]


        ## simplify Age column by bucketing
        ## group in multiples of 5*n+1 (1, 6, 11, ...)
        ## train1$Age2 <- 16*round(train1$Age/16,0)
        ## test1$Age2 <- 16*round(test1$Age/16,0)

        ## set up training control

        control1 <- trainControl(method = "cv")

        ## model using random forest

        modelfunction<-Survived~Age+Sex

        model1 <- train(modelfunction, data = train1, method="rf", trControl = control1, prox=FALSE)

        prediction1 <- predict(model1, test1)

        ## assign to zero or one. 

        prediction1[prediction1<0.75]<-0
        prediction1[prediction1>=0.75]<-1
        prediction1<-as.factor(prediction1)

        t<-table(prediction1, test1$Survived)

        conf<-confusionMatrix(test1$Survived, prediction1)


```

Here is the confusion table

```{r}
conf$table
```
 
The accuracy is `r round(100*conf$overall[1],1)`%.

###Second Approach - nested feature modeling

Age data substitution appears to be a critical factor in model accuracy. Age plays a strong role in survival. Since there are a lorge number of NAs in the age data which need to be modeled, let's see if the method of directly substituting mean age can be improved.  

Since Passenger class affects survival it's logical to look for dependency there. 

```{r "look at data", fig.align='center', echo=FALSE, warning=FALSE, message=FALSE}

        ## a key observation is that replacing age is a key swing point.
        ## survival <- age <- lots of blanks <- pay more attention here

        ## restore original data
        training<-trainingRaw
        testing<-testingRaw


        require(caret)
        set.seed(8675309)
        xx <- createDataPartition(y=training$Survived, p=0.70, list=FALSE)
  
        ##Define training and test sets

        train1<-training[xx,]
        test1<-training[-xx,]


        ## look at data

        require(ggplot2)
        p<-ggplot(train1, aes(factor(Pclass), Age), color=Sex) + geom_boxplot(colour="lightblue")
        p<-p+facet_grid(.~Sex)
        p<-p+ggtitle("Age and Passenger Class")
        p    

```

The age model is set up as follows

```{r, "age model", echo=6,warning=FALSE}

        ## to create model first get rid of NA rows in Age
        trainfull<-train1[is.na(train1$Age)==FALSE, ]
        testfull<-test1[is.na(train1$Age)==FALSE, ]
        ## fit model dependent on Pclass and Sex
        agemodel<-glm(Age~Pclass+Sex, data=trainfull)
        
        ## test model
        ## make preductions
        agePredict<-predict(agemodel, trainfull)

        ## calculate rsme
        error=agePredict-trainfull$Age
        rmseModel=sqrt(mean(error^2))

        ## compare to rsme of using just mean as predictor
        error=mean(trainfull$Age) - trainfull$Age
        rmseMean=sqrt(mean(error^2))

```

RMSE of the age-fit is `r round(rmseModel,1)` versus of using the mean `r round(rmseMean, 1)`. 

Here is a plot of the fitted age versus actual age. Note some values have been substituted...

```{r "clean data with Model", echo=FALSE}

        ## First make an extra column of predicted values

        ageTestPredict<-predict(agemodel, test1)
        test1$AgeModel<-ageTestPredict

        ageTrainPredict<-predict(agemodel, train1)
        train1$AgeModel<-ageTrainPredict


        ## Fill in NA's with Model values

        train1[is.na(train1$Age)==TRUE,"Age"]<-train1[is.na(train1$Age)==TRUE,"AgeModel"]
        test1[is.na(test1$Age)==TRUE,"Age"]<-test1[is.na(test1$Age)==TRUE,"AgeModel"]

        ## Plot the new data
                p <- ggplot(train1, aes(Age, AgeModel, color=Sex)) + geom_point()
                p <- p+ facet_grid(.~Survived)
                p <- p+ ggtitle("Age versus AgeModel / Survived")
                p 


```

The survivial model function is:  

```{r "Prediction 2", echo=6, warning=FALSE }


        control1 <- trainControl(method = "cv")
        
        ## the model function
        modelfunction <- Survived~Age+Sex+Pclass

        ## model using random forest
        model1 <- train(modelfunction, data = train1, method="rf", trControl = control1, prox=FALSE)

        prediction1 <- predict(model1, test1)

        hist(prediction1)

        plot(prediction1, test1$Survived)

        

        prediction1[prediction1<0.7]<-0
        prediction1[prediction1>=0.7]<-1
        prediction1<-as.factor(prediction1)


        conf2<-confusionMatrix(test1$Survived, prediction1)

```

Resulting in the following confusion matrix  
```{r, echo=FALSE}
conf2$table
```

The accuracy is `r round(100*conf2$overall[1],2)` versus earlier `r round(100*conf$overall[1],2)`%.

###Model4

Let's try seperate models for men and women.

```{r "look at data3", fig.align='center', echo=FALSE, warning=FALSE, message=FALSE}

        ## a key observation is that replacing age is a key swing point.
        ## survival <- age <- lots of blanks <- pay more attention here

        ## restore original data
        training<-trainingRaw
        testing<-testingRaw


        require(caret)
        set.seed(8675309)
        xx <- createDataPartition(y=training$Survived, p=0.65, list=FALSE)
  
        ##Define training and test sets

        train1<-training[xx,]
        test1<-training[-xx,]

 

```

Segregate

```{r, "segregate data"}

        train1.m<-train1[train1$Sex=="male",]
        train1.f<-train1[train1$Sex=="female",]

        test1.m<-test1[test1$Sex=="male",]
        test1.f<-test1[test1$Sex=="female",]

```


```{r, "age model3", echo=6,warning=FALSE}

        ## to create model first get rid of NA rows in Age
        trainfull.m<-train1.m[is.na(train1.m$Age)==FALSE, ]
        testfull.m<-test1.m[is.na(train1.m$Age)==FALSE, ]

        trainfull.f<-train1.f[is.na(train1.f$Age)==FALSE, ]
        testfull.f<-test1.f[is.na(train1.f$Age)==FALSE, ]

        ## fit model dependent on Pclass and Sex
        agemodel.m<-glm(Age~Pclass, data=trainfull.m)
        agemodel.f<-glm(Age~Pclass, data=trainfull.f)
     
```


```{r "clean data with Model3", echo=FALSE}

        ## First make an extra column of predicted values

        ageTestPredict.m<-predict(agemodel.m, test1.m)
        ageTestPredict.f<-predict(agemodel.f, test1.f)

        test1.m$AgeModel<-ageTestPredict.m
        test1.f$AgeModel<-ageTestPredict.f

        ageTrainPredict.m<-predict(agemodel.m, train1.m)
        ageTrainPredict.f<-predict(agemodel.f, train1.f)

        train1.m$AgeModel<-ageTrainPredict.m
        train1.f$AgeModel<-ageTrainPredict.f

        ## Fill in NA's with Model values

        train1.m[is.na(train1.m$Age)==TRUE,"Age"]<-train1.m[is.na(train1.m$Age)==TRUE,"AgeModel"]
        train1.f[is.na(train1.f$Age)==TRUE,"Age"]<-train1.f[is.na(train1.f$Age)==TRUE,"AgeModel"]

        test1.m[is.na(test1.m$Age)==TRUE,"Age"]<-test1.m[is.na(test1.m$Age)==TRUE,"AgeModel"]
        test1.f[is.na(test1.f$Age)==TRUE,"Age"]<-test1.f[is.na(test1.f$Age)==TRUE,"AgeModel"]

        ## plot the results
        require(ggplot2)        

        p <- ggplot(train1.m, aes(Age, AgeModel, color=factor(Pclass))) + geom_point()
                p <- p+ facet_grid(.~Survived)
                p <- p+ ggtitle("MALE Age versus AgeModel / Survived")
                p 

        p <- ggplot(train1.f, aes(Age, AgeModel, color=factor(Pclass))) + geom_point()
                p <- p+ facet_grid(.~Survived)
                p <- p+ ggtitle("FEMALE Age versus AgeModel / Survived")
                p 



```


Based on this plot there is a substantial population of "young" survivors who have helpers.

```{r "Prediction 3", echo=6, warning=FALSE }


        control1 <- trainControl(method = "cv")
        
        ## the model function
        modelfunction <- Survived~AgeModel+Pclass+Sex

        require(MASS)
        ## model using random forest
        model1.m <- train(modelfunction, data = train1.m, method="rf", trControl = control1, prox=FALSE)
        model1.f <- train(modelfunction, data = train1.f, method="rf", trControl = control1, prox=FALSE)

        prediction3.f <- predict(model1.f, test1.f)
        prediction3.m <- predict(model1.m, test1.m)

        hist(prediction3.f)
        hist(prediction3.m)

        ## create diagnostic plot
                test1x.m<-cbind(test1.m, prediction3.m)

        
                p <- ggplot(test1x.m, aes(prediction3.m, Survived, color=Age)) + geom_point()
                p <- p+facet_grid(.~Pclass)
                p <- p+ scale_color_gradient(low="red", high="darkgreen")
                p <- p+ggtitle("Male: Prediction vs Survival")
                p 
      


                test1x.f<-cbind(test1.f, prediction3.f)
                p <- ggplot(test1x.f, aes(prediction3.f, Survived, color=Age)) + geom_point()
                p <- p+facet_grid(.~Pclass)
                p <- p+ scale_color_gradient(low="red", high="darkgreen")
                p <- p+ggtitle("Feale: Prediction vs Survival")
                p 

        prediction3.m[prediction3.m<0.35]<-0
        prediction3.m[prediction3.m>=0.35]<-1
        #prediction3.m<-as.factor(prediction3.m)

        prediction3.f[prediction3.f<0.75]<-0
        prediction3.f[prediction3.f>=0.75]<-1
        #prediction3.f<-as.factor(prediction3.f)

       


       conf3.m<-confusionMatrix(test1.m$Survived, prediction3.m)
       conf3.f<-confusionMatrix(test1.f$Survived, prediction3.f)

```

Here is the confusion matrix
```{r, echo=TRUE}
conf3.m$table
```

Here is the confusion matrix
```{r, echo=TRUE}
conf3.f$table
```


The accuracy is `r round(100*conf3.m$overall[1],2)` for males and `r round(100*conf3.f$overall[1],2)`for females compares to the earlier `r round(100*conf2$overall[1],2)`%.

##Attempt 4

I tried an "Old Fart" model (identify old men, some of whom seemed to have survived). But that did not impove accuracy. 




```{r "look at data4", fig.align='center', echo=FALSE, warning=FALSE, message=FALSE}

        ## restore original data
        training<-trainingRaw
        testing<-testingRaw


        require(caret)
        set.seed(8675309)
        xx <- createDataPartition(y=training$Survived, p=0.70, list=FALSE)
  
        ##Define training and test sets

        train1<-training[xx,]
        test1<-training[-xx,]

 

```

The age model is set up as follows

```{r, "age model4", echo=6,warning=FALSE}

        ## to create model first get rid of NA rows in Age
        trainfull<-train1[is.na(train1$Age)==FALSE, ]
        testfull<-test1[is.na(train1$Age)==FALSE, ]
        ## fit model dependent on Pclass and Sex
        agemodel<-glm(Age~Pclass+Sex, data=trainfull)
     
```

It slightly improves the RMSE of a fit is `r round(rmseModel,1)` versus of using the mean `r round(rmseMean, 1)`. An improvement. So I'l use that to fill in blanks.

Another way to look at this data is by buckets

```{r "clean data with Model4", echo=FALSE}

        ## First make an extra column of predicted values

        ageTestPredict<-predict(agemodel, test1)
        test1$AgeModel<-ageTestPredict

        ageTrainPredict<-predict(agemodel, train1)
        train1$AgeModel<-ageTrainPredict

        ## Fill in NA's with Model values

        train1[is.na(train1$Age)==TRUE,"Age"]<-train1[is.na(train1$Age)==TRUE,"AgeModel"]
        test1[is.na(test1$Age)==TRUE,"Age"]<-test1[is.na(test1$Age)==TRUE,"AgeModel"]


```


```{r "old farts4"}

        train1$OldFarts<- -1
        train1$OldFarts[train1$Sex=="male"&train1$Age>60] <- 1

        test1$OldFarts<- -1
        test1$OldFarts[test1$Sex=="male"&test1$Age>60] <- 1

```


```{r "Prediction 4", echo=6, warning=FALSE }


        control1 <- trainControl(method = "cv")
        
        ## the model function
        modelfunction <- Survived~Age+Pclass+OldFarts+Sex

        ## model using random forest
        model1 <- train(modelfunction, data = train1, method="rf", trControl = control1, prox=FALSE)

        prediction4 <- predict(model1, test1)

        hist(prediction4)

        ## create diagnostic plot
                test1x<-cbind(test1, prediction4)

        
                p <- ggplot(test1x, aes(prediction4, Survived, color=Age)) + geom_point()
                p <- p+ facet_grid(.~Sex)
                p <- p+ scale_color_gradient(low="red", high="darkgreen")
                p <- p+ ggtitle("Prediction vs Survival / Helper")
                p 
      
        

        prediction4[prediction4<0.8]<-0
        prediction4[prediction4>=0.8]<-1
        prediction4<-as.factor(prediction4)

       


        conf4<-confusionMatrix(test1$Survived, prediction4)

```

Here is the confusion matrix
```{r, echo=FALSE}
conf4$table
```

The accuracy is `r round(100*conf4$overall[1],2)` compares to earlier `r round(100*conf2$overall[1],2)`%.

